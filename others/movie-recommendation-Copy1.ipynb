{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos\n",
    "\n",
    "Funcion para cargar las sinopsis de las películas y los metadados de las mismas (género, pais, etc). Nos quedaremos con las películas para las cuales tengamos tanto los metadatos como la sinpsis. La lista de películas será un objeto de tipo **list** y los datos de cada película un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movies(max_movies=90000):\n",
    "    def load_plot_summaries():\n",
    "        # plot_summaries_file = open(\"data/plot_summaries.fake.txt\", \"r\")\n",
    "        plot_summaries_file = open(\"data/plot_summaries.txt\", \"r\", encoding=\"utf8\")\n",
    "        plot_summaries = dict()\n",
    "        for plot_summary_line in plot_summaries_file:\n",
    "            plot_summary_data = plot_summary_line.split('\\t')\n",
    "            # Summaries structure\n",
    "            # [0] Wikipedia movie ID\n",
    "            # [1] Summary plot\n",
    "            plot_summary = dict()\n",
    "            plot_summary['id'] = plot_summary_data[0]\n",
    "            plot_summary['summary'] = plot_summary_data[1]\n",
    "\n",
    "            plot_summaries[plot_summary['id']] = plot_summary\n",
    "\n",
    "        return plot_summaries\n",
    "\n",
    "    print(\"Cargando datos de peliculas...\")\n",
    "\n",
    "    plot_summaries = load_plot_summaries()\n",
    "\n",
    "    # metadata_file = open(\"data/movie.metadata.fake.tsv\", encoding=\"utf8\")\n",
    "    metadata_file = open(\"data/movie.metadata.tsv\", \"r\", encoding=\"utf8\")\n",
    "    movies = []\n",
    "\n",
    "    movies_count = 0\n",
    "\n",
    "    for metadata_line in metadata_file:\n",
    "        movie_metadata = metadata_line.split('\\t')\n",
    "\n",
    "        # Metadata structure\n",
    "        # [0] Wikipedia movie ID\n",
    "        # [1] Freebase movie ID\n",
    "        # [2] Movie name\n",
    "        # [3] Movie release date\n",
    "        # [4] Movie box office revenue\n",
    "        # [5] Movie runtime\n",
    "        # [6] Movie languages (Freebase ID:name tuples)\n",
    "        # [7] Movie countries (Freebase ID:name tuples)\n",
    "        # [8] Movie genres (Freebase ID:name tuples)\n",
    "\n",
    "        id = movie_metadata[0]\n",
    "\n",
    "        # Añadimos la pelicula solo si tiene sinopsis, incluimos una lista con las claves de los generos\n",
    "        if (id in plot_summaries) & (movies_count < max_movies):\n",
    "            movies_count += 1\n",
    "            movie = dict()\n",
    "            movie['id'] = id\n",
    "            movie['name'] = movie_metadata[2]\n",
    "            movie['date'] = movie_metadata[3]\n",
    "            movie['genres'] = list(json.loads(movie_metadata[8].replace(\"\\\"\\\"\", \"\\\"\").replace(\"\\\"{\", \"{\").replace(\"}\\\"\", \"}\")).keys())\n",
    "            movie['summary'] = plot_summaries[id].get('summary')\n",
    "            movies.append(movie)\n",
    "\n",
    "    print(\"Número de películas cargadas:\", len(movies))\n",
    "    \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la funcíon anterior cargamos los datos, dado que el proceso es bastante pesado solo cargaremos un subconjunto de las mismas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de peliculas...\n",
      "Número de películas cargadas: 100\n"
     ]
    }
   ],
   "source": [
    "movies = load_movies(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado de las sinópsis\n",
    "\n",
    "A continuación procesamos las sinopsis, quedándonos con las palabras diferentes que encontramos en cada una de ellas, adicionalmente las vamos a transformar en sus raices (stemmer) y obviaremos las **stopwords** y los nomrbres propios, dado que no aportan significado.\n",
    "\n",
    "Primero definimos una función para hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_global_texts(movies):\n",
    "    def get_words(text):\n",
    "        def add_word(word):\n",
    "            word = word.lower()\n",
    "            if word not in stop_words:\n",
    "                words.append(stemmer.stem(word))\n",
    "\n",
    "\n",
    "        words = []\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(tokenizer.tokenize(text))):\n",
    "            # nltk.word_tokenize    devuelve la lista de palabras que forman la frase (tokenización)\n",
    "            # nltk.pos_tag          devuelve el part of speech (categoría) correspondiente a la palabra introducida\n",
    "            # nltk.ne_chunk         devuelve la etiqueta correspondiente al part of speech (POC)\n",
    "            try:\n",
    "                if chunk.label() == 'PERSON':\n",
    "                    # PERSON es un POC asociado a los nombres propios, los cuales no vamos a añadir\n",
    "                    pass\n",
    "                else:\n",
    "                    for c in chunk.leaves():\n",
    "                        add_word(c[0])\n",
    "            except AttributeError:\n",
    "                add_word(chunk[0])\n",
    "\n",
    "        return words\n",
    "\n",
    "    print(\"Extrayendo palabras de los textos...\")\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    global_texts = []\n",
    "\n",
    "    [global_texts.append(get_words(movie['summary'])) for movie in movies]\n",
    "\n",
    "    return global_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la ejecutamos, tendremos una lista de listas, en la que para cada película tendremos las palabras que definen su sinopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo palabras de los textos...\n"
     ]
    }
   ],
   "source": [
    "global_texts = get_global_texts(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del diccionaro\n",
    "\n",
    "El diccionario está formado por la concatenación de todas las palabras que aparecen en alguna sinopsis (modo texto) de alguna de las peliculas. Básicamente esta función mapea cada palabra única con su identificador. Es decir, si tenemos N palabras, lo que conseguiremos al final es que cada película sea representada mediante un vector en un  espacio de N dimensiones.\n",
    "\n",
    "Para ello, partiendo de la lista creada en el paso anterior, usaremos la función **corpora** del paquete **gensim**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x291b3a7a748>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(global_texts)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x291b3a7a6d8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Corups\n",
    "\n",
    "Crearemos un corpus con la colección de todos los resúmenes previamente pre-procesados y transformados usando el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in global_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, mostramos las 5 primeras entradas de la primera pelicula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creación del TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tfidf(corpus):\n",
    "    print(\"Creación del Modelo Espacio-Vector Tf-Idf\")\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf\n",
    "\n",
    "corpus_tfidf = create_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo LSI\n",
    "\n",
    "Para ello vamos a definir una función auxiliar y posteriormente la invocaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsi_model(corpus_tfidf, dictionary, total_lsa_topics):\n",
    "    print(\"Creación del modelo LSA: Latent Semantic Analysis\")\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=total_lsa_topics)\n",
    "    similarity_matrix = similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
    "    return lsi, similarity_matrix\n",
    "\n",
    "(lsi, similarity_matrix) = create_lsi_model(corpus_tfidf, dictionary, TOTAL_LSA_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}