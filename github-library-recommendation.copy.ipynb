{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #8B008B; font-family: Arial; font-size: 3em;\">Sistema de recomendación de librerías Python</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:#4B0082; font-family: Arial; font-size: 2em;\">José Pérez Yázquez</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El presente proyecto implementa una propuesta de recomendador de librerías Python, la idea es que dado un proyecto que estemos desarrollando, nos recomiende librerías que otros proyectos similares han usado. Para ello usa como \"corpus\" la BBDD por excelencia de código, **Github**, y como medida de similitud sus descripciones y las librerías que ya compartan.\n",
    "\n",
    "El proyecto consta de dos fases, en la primera de ellas obtendremos todos los metadatos de los proyectos usando la API Rest de Github y en la segunda parte, basándonos en esos metadatos construiremos el sistema de recomendación \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga y procesamiento de datos\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta fase se divide a su vez en tres subfases:\n",
    " 1. Recopilación de la información de los proyectos\n",
    " 2. Clonado de los proyectos\n",
    " 3. Procesamiento de los proyectos clonados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recopilación de la información de los proyectos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la API Rest de Github descarga la información, metadatos, de todos los proyectos que cumplen una serie de criterios, estos serán los siguientes:\n",
    "  - Identificados como proyectos Python\n",
    "  - Creado entre el 1 de Enero de 2012 y hasta la fecha actual\n",
    "  - Con un mínimo de 10 estrellas\n",
    "\n",
    "Toda la información es almacenada en MongoDB concretamente en una colección llamada **projects** de una BD llamada **github**, la información almacenada es la siguiente:\n",
    " - id: \n",
    " - name\n",
    " - full_name\n",
    " - created_at\n",
    " - git_url\n",
    " - description\n",
    " - language\n",
    " - stars\n",
    " - done: Se usa a nivel interno, indica si un proyecto está procesado, esto es identificados las librerías que usa y el fichero README cargado, en los puntos siguientes se detallan estos dos datos\n",
    " - readme_txt: Almacenará el texto de los ficheros README de los proyectos, será la información base para el sistema de recomendación\n",
    " - library: Lista con las librerías que el proyecto usa, en principio estará vacía y se alimentará cuando se procese el proyecto\n",
    " - raw_data: toda la información del proyecto sin procesar por si más adelante es necesario algún dato que en estos momentos no parece relevante\n",
    "\n",
    "Cuando se usa la API de Github sin autenticación existen una serie de límites que no podemos superar, el primero es que el número máximo de resultados que cualquier consulta puede devolver\n",
    "es de 1000, para salvar este impedimento, vamos a realizar sucesivas consultas restringiendo las llamadas a un solo día del intervalo que vamos a cubrir.\n",
    "\n",
    "Por otra parte, existe otra restricción en el uso de la API, el número de llamadas por minuto está limitado a 60, podríamos incrementar esta cantidad si las llamadas son autenticadas,\n",
    "esto es usar por ejemplo un \"client ID\" y \"secret\"  como partes de la consulta, pero vamos abordar una estrategia diferente. Se irán realizando las consultas sin atender a los límites y cuando ese límite se supere, se suspenderá el proceso durante 61 segundos y posteriormente se reanudará\n",
    "por el punto en el que iba.\n",
    "\n",
    "Si se desea consultar los límites comentados, podemos hacer una consulta, por ejemplo usando curl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -i https://api.github.com/users/octocat\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "Date: Mon, 01 Jul 2013 17:27:06 GMT\n",
    "Status: 200 OK\n",
    "X-RateLimit-Limit: 60\n",
    "X-RateLimit-Remaining: 56\n",
    "X-RateLimit-Reset: 1372700873\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la cabecera de la respuesta nos muestra los límites con el valor actual:\n",
    "\n",
    " - **X-RateLimit-Limit**       Número máximo de solicitudes que se le permite hacer por hora.\n",
    " - **X-RateLimit-Remaining**   Número de solicitudes restantes en la ventana de límite de velocidad actual.\n",
    " - **X-RateLimit-Reset**       Hora a la que se restablece la ventana de límite de velocidad actual en segundos UTC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a cargar las librerías necesarias para esta fase del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from time import sleep\n",
    "from pymongo import MongoClient\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a definir una serie de constantes que serán usadas en el código, de esta manera podremos cambiar facilimente alguno de estos valores.\n",
    "El significado de cada una de ellas es el siguiente:\n",
    " - MIN_STARTS: Número mínimo de estrellas que tiene que tener un proyecto para ser considerado\n",
    " - START_DATE: Fecha de inicio del intervalo, haciendo referencia a la fecha de creación del proyecto en Github\n",
    " - END_DATE: Fecha de fin del intervalo\n",
    " - URL_PATTERN: Patrón de la URL para la consulta a la API de Github, podemos ver que filtramos por lenguaje (Python), por fecha de creación (la suministraremos durante la ejecución) y por numero de estrellas (referido a la constante comentada anteriormente).\n",
    " - ROOT_PATH: Directorio raiz donde clonaremos los repositorios\n",
    " - CLONE_COMMAND: Comando usado para clonar los repositorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_STARTS = 10\n",
    "START_DATE = date(2012, 1, 1)\n",
    "END_DATE = date(2017, 8, 28)\n",
    "URL_PATTERN = 'https://api.github.com/search/repositories?q=language:Python+created:{0}+stars:>={1}&type=Repositories'\n",
    "ROOT_PATH = \"d:/tmp/\"\n",
    "CLONE_COMMAND = \"git clone {0} {1}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha comentado anteriormente, vamos a usar MongoDB como tecnología de persistencia, vamos a definir una funcion que nos devuelva una referencia a la colección de proyectos con la que estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_repository():\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.github\n",
    "    return db.projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función es la que realmente hace las llamadas a la API, tendrá como entrada la fecha de creación de los proyectos, usará la URL que hemos definicdo como constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_projects_by_date(date):\n",
    "    print(\"Processing date \", date)\n",
    "    url_pattern = URL_PATTERN\n",
    "    url = url_pattern.format(date, MIN_STARTS)\n",
    "    response = requests.get(url)\n",
    "    if (response.ok):\n",
    "        response_data = json.loads(response.content.decode('utf-8'))['items']\n",
    "        for project in response_data:\n",
    "            insert_project(project)\n",
    "    else:\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuanción definimos otra función auxiliar, que dada la información de la respuesta de la API, seleccionamos los atributos que vamos a necesitar y los almacenacomo un documento Mongo. Adicionalmente crea las propiedades mencionadas previamente (readme_txt,library,etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_project(github_project):\n",
    "    if projects.find_one({\"id\": github_project[\"id\"]}):\n",
    "        print(\"Project {0} is already included in the repository\".format(github_project[\"name\"]))\n",
    "    else:\n",
    "        project = {\n",
    "            'id': github_project[\"id\"],\n",
    "            'name': github_project[\"name\"],\n",
    "            'full_name': github_project[\"full_name\"],\n",
    "            'created_at': github_project[\"id\"],\n",
    "            'git_url': github_project[\"git_url\"],\n",
    "            'description': github_project[\"description\"],\n",
    "            'language': github_project[\"language\"],\n",
    "            'stars': github_project[\"stargazers_count\"],\n",
    "            'done': False,\n",
    "            'readme_txt': \"\",\n",
    "            'library': [],\n",
    "            'raw_data': github_project\n",
    "        }\n",
    "        projects.insert(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente ejecutamos el programa que, usando las funciones anteriormente definidas, descarga la información de los proyectos y la inserta en la BBDD, podemos observar que descompone las llamadas para traer en cada una de ellas solo los proyectos de un determinado dia y que *\"gestiona\"* las restricciones que tenemos en cuanto a llamada en la ventana de tiempo actual. En primer lugar recuperamos la colección en la que vamos a insertar los proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MongoClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2e0a7bff792d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprojects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_repository\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-dbe7298e9681>\u001b[0m in \u001b[0;36mget_repository\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_repository\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMongoClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'localhost'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m27017\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgithub2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MongoClient' is not defined"
     ]
    }
   ],
   "source": [
    "projects = get_repository()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Y cargamos los proyectos en la colección, aplicamos las técnicas descritas para salvar las restricciones que nos impone la API de Github (número de items devuelto por cada consulta y número de llamadas por minuto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'END_DATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7e84a80dfa9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mproject_create_at\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSTART_DATE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEND_DATE\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mSTART_DATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mget_projects_by_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject_create_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> Reached call limit, waiting 61 seconds...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'END_DATE' is not defined"
     ]
    }
   ],
   "source": [
    "for project_create_at in [START_DATE + timedelta(days=x) for x in range((END_DATE - START_DATE).days + 1)]:\n",
    "    try:\n",
    "        get_projects_by_date(project_create_at)\n",
    "    except:\n",
    "        print(\">> Reached call limit, waiting 61 seconds...\")\n",
    "        sleep(61)\n",
    "        get_projects_by_date(project_create_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto tendriamos cargada toda la información que necesitamos de Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Clonado de los proyectos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar las librerias que un proyecto utiliza, un camino podría ser buscar los ficheros **requirements** de cada proyecto y parsear esa información, pero dicho fichero no está presente en una gran cantidad de proyectos, por lo que se ha optado por un camino un poco más **radical**.\n",
    "\n",
    "Usando la información cargada en el paso anterior, fundamentalmente la propiedad **git_url**, vamos a clonar cada proyecto en local para posteriormente procesarlos.\n",
    "\n",
    "**NOTA**: Filtramos por los proyectos que no están procesados y nos aseguramos que el directorio no existe, porque este código, dado que puede tardar bastante, lo vamos a ejecutar en diferentes momentos, es decir llegado un punto se podría abortar el proceso y lanzarlo más adelante sin que se resienta por ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5401b451f8e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mROOT_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprojects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'done'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mROOT_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir(ROOT_PATH)\n",
    "\n",
    "for project in projects.find({'done':False}):\n",
    "    path = ROOT_PATH + \"/\" + str(project[\"id\"])\n",
    "    if not os.path.isdir(path):\n",
    "        os.system(CLONE_COMMAND.format(project[\"git_url\"], project[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En este punto tendríamos clonados todos los repositorios con los que construiremos el sistema de recomendación.\n",
    "\n",
    "**NOTA**: Son muchos, muchos gigabytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento de los proyectos clonados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comenzamos esta etapa definiendo una función que dado un fichero python (extensión **.py**), lo recorre linea por linea y, usando expresiones regulares, buscamos todas las librerias que se estén usando.\n",
    "Almacenamos esta lista en la propiedad **library** del proyecto en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_python_file(project, file_path):\n",
    "    def add_to_list(item):\n",
    "        if not item in library:\n",
    "            library.append(item)\n",
    "\n",
    "    library = project['library']\n",
    "    pattern = '(?m)^(?:from[ ]+(\\S+)[ ]+)?import[ ]+(\\S+)[ ]*$'\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                if match.group(1) != None:\n",
    "                    add_to_list(match.group(1))\n",
    "                else:\n",
    "                    add_to_list(match.group(2))\n",
    "    project['library'] = library\n",
    "    projects.update({'_id': project['_id']}, {\"$set\": project}, upsert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte, definimos también una función que dado un fichero **README** lo lea y almacena en la propiedad **readme_txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_readme_file(project, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        project['readme_txt'] = f.read()\n",
    "        projects.update({'_id': project['_id']}, {\"$set\": project}, upsert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente recorremos los repositorios no procesados, como comentabamos anteriormente este proceso podría ser necesario lanzarlo reiteradas veces, para cada repositorio analizamos su fichero **README** y cada uno de los ficheros Python para extraer las librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d215ed5c84ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprojects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'done'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mROOT_PATH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'projects' is not defined"
     ]
    }
   ],
   "source": [
    "for project in projects.find({'done':False}):\n",
    "    try:\n",
    "        print(\".\",end=\"\")\n",
    "        path = ROOT_PATH.format(project[\"id\"])\n",
    "        if os.path.isdir(path):\n",
    "            print(\"Processing project: \", project[\"id\"])\n",
    "            project['done'] = True\n",
    "            projects.update({'_id': project['_id']}, {\"$set\": project}, upsert=False)\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                for file in files:\n",
    "                    try:\n",
    "                        if file.endswith(\".py\"):\n",
    "                            process_python_file(project, os.path.join(root, file))\n",
    "                        else:\n",
    "                            if file.lower().startswith(\"readme.\"):\n",
    "                                process_readme_file(project, os.path.join(root, file))\n",
    "                    except:\n",
    "                        pass\n",
    "    except:\n",
    "        print(\"Error procesing project {0} [{1}] - {2}\".format(project['id'], project['name'], sys.exc_info()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, tendríamos una colección MongoDB con cada uno de los proyectos de nuestro **pre-corpus** con la lista de librerías que cada proyecto usa así como su descripción **extendida** extraida de su fichero READMA. Estaríamos en disposición por tanto de comenzar la implementación de nuestro recomendador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del recomendador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principios teóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para identificar la similitud entre los proyectos basándonos en su descripción, entendiéndose como tal su fichero README, utilizamos el \"análisis semántico latente\" (LSA, usando la abreviatura en inglés), que es una técnica ampliamente utilizada en el procesamiento del lenguaje natural. LSA transforma cada texto en un vector, en un espacio de características. En nuestro caso, las características son palabras que ocurren en las descripciones. A continuación, se crea una matriz que contiene todos los vectores: las columnas representan las descripciones de los proyectos y las filas representan palabras únicas. Por consiguiente, el número de filas puede ascender a decenas de miles de palabras. \n",
    "\n",
    "Con el fin de identificar las características relevantes de esta matriz, usaremos la \"descomposición de valores singulares\" (SVD, usando la abreviatura en inglés), que es una técnica de reducción de dimensión, se utiliza para reducir el número de líneas -palabras-, manteniendo y resaltando la similitud entre columnas-descripción -. La dimensión de esta matriz de aproximación se establece mediante un hiperparámetro que es el número de temas, comúnmente llamado como tópicos. En este marco, un tópico consiste en un conjunto de palabras con pesos asociados que definen la contribución de cada palabra a la dirección de este tópico. Basándose en esta matriz de aproximación de baja dimensión, la similitud entre dos columnas -descripciones- se calcula utilizando el coseno del ángulo entre estos dos vectores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del recomendador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Importación de librerías y módulos\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comenzamos importando las librerías que vamos a usar en esta fase del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim import corpora, models, similarities, matutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de los datos\n",
    "\n",
    "Definimos una funcion para cargar los datos de los proyectos, entre los datos a cargar está el fichero **readme** y las librerías usadas, datos que serán los que usemos para calcular las similitudes entre los proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_projects(max_projects=50000):\n",
    "    def get_repository():\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        db = client.github\n",
    "        return db.projects\n",
    "\n",
    "    projects_repository = get_repository()\n",
    "\n",
    "    return list(projects_repository.find({'done': True}).limit(max_projects))\n",
    "\n",
    "\n",
    "def load_project_old(max_projects=90000):\n",
    "    def load_plot_summaries():\n",
    "        plot_summaries_file = open(\"data/plot_summaries.txt\", \"r\", encoding=\"utf8\")\n",
    "        # plot_summaries_file = open(\"data/plot_summaries.txt\", \"r\", encoding=\"utf8\")\n",
    "        plot_summaries = dict()\n",
    "        for plot_readme_txt_line in plot_summaries_file:\n",
    "            plot_readme_txt_data = plot_readme_txt_line.split('\\t')\n",
    "            # Summaries structure\n",
    "            # [0] Wikipedia project ID\n",
    "            # [1] Summary plot\n",
    "            plot_readme_txt = dict()\n",
    "            plot_readme_txt['id'] = plot_readme_txt_data[0]\n",
    "            plot_readme_txt['readme_txt'] = plot_readme_txt_data[1]\n",
    "\n",
    "            plot_summaries[plot_readme_txt['id']] = plot_readme_txt\n",
    "\n",
    "        return plot_summaries\n",
    "\n",
    "    print(\"Cargando datos de proyectos...\")\n",
    "\n",
    "    plot_summaries = load_plot_summaries()\n",
    "\n",
    "    metadata_file = open(\"data/movie.metadata.tsv\", encoding=\"utf8\")\n",
    "    # metadata_file = open(\"data/project.metadata.tsv\", \"r\", encoding=\"utf8\")\n",
    "    projects = []\n",
    "\n",
    "    projects_count = 0\n",
    "\n",
    "    for metadata_line in metadata_file:\n",
    "        project_metadata = metadata_line.split('\\t')\n",
    "        id = project_metadata[0]\n",
    "\n",
    "        # Añadimos la proyecto solo si tiene sinopsis, incluimos una lista con las claves de los generos\n",
    "        if (id in plot_summaries) & (projects_count < max_projects):\n",
    "            projects_count += 1\n",
    "            project = dict()\n",
    "            project['id'] = id\n",
    "            project['name'] = project_metadata[2]\n",
    "            project['date'] = project_metadata[3]\n",
    "            project['libraries'] = list(json.loads(project_metadata[8].replace(\"\\\"\\\"\", \"\\\"\").replace(\"\\\"{\", \"{\").replace(\"}\\\"\", \"}\")).values())\n",
    "            project['readme_txt'] = plot_summaries[id].get('readme_txt')\n",
    "            projects.append(project)\n",
    "\n",
    "    print(\"Número de proyectos cargadas:\", len(projects))\n",
    "\n",
    "    return projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la funcíon anterior cargamos los datos, dado que el proceso es bastante pesado solo cargaremos un subconjunto de las mismas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projects = load_projects(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, mostramos las primeras 5 librerías de uno de los proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calendar', 'select', 'struct', 'sys', 'termios']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects[2]['library'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y el contenido del fichero README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# About #\\n\\n**LivelierView** is an attempt to reverse-engineer the LiveView Bluetooth protocol.\\n\\n# Current state #\\n\\nThe Python server can currently:\\n \\n * Identify LiveView devices\\n * Set the time of the device\\n * Keep it awake\\n * Make it vibrate\\n * Flash the LED\\n\\n\\n# Requirements #\\n\\nThe server requires:\\n\\n * dbus-python\\n * python-gobject (for an event loop that's not currently being used properly - pairing has not yet been implemented)\\n * bluez (I'm running 4.77 and that works but I would think any version with\\n   the same DBus API would work)\\n * pyserial (available from PyPi) - I'd like to drop this dependency but I\\n   haven't bothered to figure out how to do non-blocking reading with Python\\n\\nThe device must already be paired with your computer. To enter pairing mode,\\nturn off the LiveView device, hold power and keep it pressed while it's\\nbooting.\\n\\nAs a side effect of all the experimentation, I found out that the LiveView\\nwill only ever hold one pairing code in memory - if you pair it with your\\ncomputer, you'll have to re-pair it with your phone afterwards.\\n\\n# Usage #\\n\\nEither run the script as root, or setup a udev rule to make `/dev/rfcomm0`\\nworld read-/writeable. The latter is left as an exercise for the reader ;)\\n\\n# TO DO #\\n\\n* Figure out if the main menu is part of the firmware (judging by the Manager's\\nresources the icons are coming from the phone) and customize it. \\n* Basic navigation/notification\\n\\nVery far into the future:\\n\\n* A less sandbox-y plugin system where plugins can spontaneously take over the\\n  watch.\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects[2]['readme_txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de las descripciones\n",
    " \n",
    "A continuación procesamos las descripciones, quedándonos con las palabras diferentes que encontramos en cada una de ellas, en primer lugar, eliminamos puntuaciones y palabras irrelevantes como nombres personales y palabras comunes que no aportan significado (denominados \"stop words\"). Esto evitará que se formen tópicos en torno a ellos\n",
    "\n",
    "Además, utilizamos el stemmer Snowball, también llamado Porter2 stemmer, para detectar palabras similares presentes en diferentes formatos (eliminar sufijo, prefijo, etc.). Snowball es un lenguaje desarrollado por M.F. Porter, para definir de forma eficiente stemmers. Este algoritmo de derivación es el más utilizado en el dominio del procesamiento del lenguaje natural.\n",
    "\n",
    "Para hacer el procesamietno usaremos la librería nltk( Natural Language Toolkit), proporciona un gran número de métodos que cubren diferentes temas en el dominio de los datos del lenguaje humano, como la clasificación, derivación, etiquetado, análisis y razonamiento semántico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que dado un texto lo descompone en las palabras con significado que lo componen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    def add_word(word):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            words.append(stemmer.stem(word))\n",
    "\n",
    "    words = []\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(tokenizer.tokenize(text))):\n",
    "        # nltk.word_tokenize    devuelve la lista de palabras que forman la frase (tokenización)\n",
    "        # nltk.pos_tag          devuelve el part of speech (categoría) correspondiente a la palabra introducida\n",
    "        # nltk.ne_chunk         devuelve la etiqueta correspondiente al part of speech (POC)\n",
    "        try:\n",
    "            if chunk.label() == 'PERSON':\n",
    "                # PERSON es un POC asociado a los nombres propios, los cuales no vamos a añadir\n",
    "                pass\n",
    "            else:\n",
    "                for c in chunk.leaves():\n",
    "                    add_word(c[0])\n",
    "        except AttributeError:\n",
    "            add_word(chunk[0])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que aplica la función anterior a los resúmenes de todos los proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_from_readme(projects):\n",
    "    texts = []\n",
    "    [texts.append(get_words(project['readme_txt'])) for project in projects]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente ejecutamos la función que acabamos de definir, tendremos una lista de listas, en la que para cada proyecto tendremos las palabras que lo definen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts_from_readme = get_texts_from_readme(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, mostramos las 5 primeras entradas de uno de los proyectos, es decir las 5 primeras palabras con significado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['livelierview', 'attempt', 'revers', 'engin', 'liveview']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_from_readme[2][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del diccionario\n",
    " \n",
    "El diccionario está formado por la concatenación de todas las palabras que aparecen en algún resumen de alguno de los proyectos. Básicamente esta función mapea cada palabra única con su identificador. Es decir, si tenemos N palabras, lo que conseguiremos al final es que cada proyecto sea representada mediante un vector en un espacio de N dimensiones.\n",
    " \n",
    "Para ello, partiendo de la lista creada en el paso anterior, usaremos la función **corpora** del paquete **gensim**.\n",
    "\n",
    "El diccionario consiste en una concatenación de palabras únicas de todas las descripciones. Gensim es una biblioteca eficiente para analizar la similitud semántica latente entre documentos.\n",
    "Este módulo implementa el concepto de Diccionario - un mapeo entre palabras y\n",
    "sus entes ids.\n",
    "\n",
    "Los diccionarios pueden ser creados a partir de un corpus y luego pueden ver las frecuencia del documento (eliminación de palabras comunes mediante el método func: `Dictionary.filter_extremes`), guardado / cargado desde el disco (vía: func: `Dictionary.save` y: func:` Dictionary.load`), fusionado con otro diccionario (: func: `Dictionary.merge_with`) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1ef3932df28>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts_from_readme)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/lda_training_tips.ipynb\n",
    "\n",
    "We remove rare words and common words based on their document frequency. Below we remove words that appear in less than 20 documents or in more than 50% of the documents. Consider trying to remove words only based on their frequency, or maybe combining that with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#POC\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la longitud del diccionario creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3249 unique tokens: ['anew', 'pars', '1841', 'alia', 'roll']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función **token2i** asigna palabras únicas con sus ids. En nuestro caso, la longitud del diccionario es igual a *N* palabras lo que significa que cada descripción del proyecto será representada a través de un espacio vectorial de *N* dimensiones\n",
    "\n",
    "Mostramos las primeras 10 entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anew', 0),\n",
       " ('pars', 1),\n",
       " ('1841', 2),\n",
       " ('alia', 3),\n",
       " ('roll', 4),\n",
       " ('contamin', 2804),\n",
       " ('server_arg', 5),\n",
       " ('recal', 7),\n",
       " ('develop', 1019),\n",
       " ('mem', 8)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(dictionary.token2id.items(), 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Corpus\n",
    "\n",
    "Crearemos un corpus con la colección de todos los resúmenes previamente pre-procesados y transformados usando el diccionario. Vamos a convertir los textos a un formato que gensim puede utilizar, esto es, una representación como bolsa de palabras (BoW). Gensim espera ser alimentado con una estructura de datos de corpus, básicamente una lista de \"sparce vectors\", estos constan de pares (id, score), donde el id es un ID numérico que se asigna al término a través de un diccionario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many tokens and documents we have to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3249\n",
      "Number of documents: 100\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(texts_from_readme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_corpus(texts):\n",
    "    return [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = create_corpus(texts_from_readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, mostramos las 5 primeras entradas del primer proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 2), (10, 1), (15, 1), (17, 6), (44, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'seek'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-23d480a1e1f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextCorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMmCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\textcorpus.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             logger.warning(\"No input document stream provided; assuming \"\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m \u001b[0munique\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[1;31m# log progress & run a regular check for pruning, once every 10k docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdocno\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\textcorpus.py\u001b[0m in \u001b[0;36mget_texts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# assume documents are lines in a single file (one document per line).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# Yield each document as a list of lowercase tokens, via `utils.tokenize`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mfile_or_filename\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# input already a file-like object; just reset to the beginning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'seek'"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import TextCorpus, MmCorpus\n",
    "xc = TextCorpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del TFID\n",
    "\n",
    "Un alto peso en tf-idf se alcanza por una alta frecuencia en un Documento y una baja frecuencia en toda la colección de documentos; los pesos tienden a filtrar términos comunes. Para la creación de este corpus, vamos a usar la función **TfidfModel** del objeto **models** (perteneciente a la librería *gemsim*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creación del Modelo Espacio-Vector Tf-Idf\n"
     ]
    }
   ],
   "source": [
    "def create_tfidf(corpus):\n",
    "    print(\"Creación del Modelo Espacio-Vector Tf-Idf\")\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf\n",
    "\n",
    "\n",
    "corpus_tfidf = create_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En corpus tenemos, para cada project, una lista con sus palabras y el tfidf de cada una. Mostramos el primer elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.03372610040252964),\n",
       " (10, 0.011563474235802425),\n",
       " (15, 0.01837015935790864),\n",
       " (17, 0.04234230542532715),\n",
       " (44, 0.01686305020126482),\n",
       " (67, 0.008842782659579222),\n",
       " (71, 0.012614748095252884),\n",
       " (93, 0.01837015935790864),\n",
       " (95, 0.024125570620564394),\n",
       " (125, 0.0427532444554839)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos saber que palabra es cada uno de estos términos podemos consultar el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anew , pars , 1841\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[0],\",\",dictionary[1],\",\", dictionary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3249 unique tokens: ['anew', 'pars', '1841', 'alia', 'roll']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Modelo LSI\n",
    "\n",
    "Para ello vamos a definir una función auxiliar y posteriormente la invocaremos, además de crear el modelo LSI, vamos a usarlo para crear la matriz de similitudes. Antes de nada vamos a definir una serie de constantes para controlar el proceso.\n",
    " \n",
    "* **TOTAL_LSA_TOPICS**\n",
    "Limita el numero de terminos, por supuesto tiene que ver con el tamaño de la muestra, mientras más proyectos tengamos mas terminos tendremos y por tanto la reduccion seria mayor, estamos clusterizando las proyectos en TOTAL_TOPICOS_LSA clusters\n",
    " \n",
    "* **SIMILARITY_THRESHOLD** Umbral de similitud que se debe superar para que dos proyectos se consideren similares\n",
    "\n",
    "* **LIBRARY_COINCIDENCE_RATE** porcentaje en el que incrementaremos la similitud de los proyectos por cada librería que dos proyectos tengan en común\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinación del número de topicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/lda_training_tips.ipynb\n",
    "\n",
    "Training\n",
    "\n",
    "We are ready to train the LDA model. We will first discuss how to set some of the training parameters.\n",
    "First of all, the elephant in the room: how many topics do I need? There is really no easy answer for this, it will depend on both your data and your application. I have used 10 topics here because I wanted to have a few topics that I could interpret and \"label\", and because that turned out to give me reasonably good results. You might not need to interpret all your topics, so you could use a large number of topics, for example 100.\n",
    "The chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. I've set chunksize = 2000, which is more than the amount of documents, so I process all the data in one go. Chunksize can however influence the quality of the model, as discussed in Hoffman and co-authors [2], but the difference was not substantial in this case.\n",
    "passes controls how often we train the model on the entire corpus. Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough.\n",
    "I suggest the following way to choose iterations and passes. First, enable logging (as described in many Gensim tutorials), and set eval_every = 1 in LdaModel. When training the model look for a line in the log that looks something like this:\n",
    "2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations\n",
    "\n",
    "If you set passes = 20 you will see this line 20 times. Make sure that by the final passes, most of the documents have converged. So you want to choose both passes and iterations to be high enough for this to happen.\n",
    "We set alpha = 'auto' and eta = 'auto'. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Como se mencionó anteriormente, LSA busca identificar un conjunto de topicos relacionados las descripciones de los proyectos. El número de estos temas N es igual a la dimensión de la matriz de aproximación resultante de la técnica de reducción de dimensión SVD. Este número es un hiperparámetro que se debe ajustar cuidadosamente, es el resultado de la selección de los N valores singulares más grandes de la matriz del corpus tf-idf. Estos valores singulares se pueden calcular de la siguiente manera:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar usando la función **corpus2dense** , convertimos el corpus en una matriz np densa (los documentos serán columnas). Esta función necesita el número de características (parámetro **num_terms**), porque la dimensionalidad no se puede deducir solo con el corpus. Adicionalmente vamos a suministrar el numero de documentos (parámetro **num_docs**), para que el algoritmo sea más eficiente.\n",
    "\n",
    "*Nota: La función inversa de **corpus2dense** es **Dense2Corpus** *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs = len(texts_from_readme)\n",
    "num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3250"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_terms = len(dictionary)\n",
    "num_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  0.,  0., ...,  2.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  4.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_matrix = matutils.corpus2dense(corpus, num_terms=num_terms, num_docs=num_docs)\n",
    "numpy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = np.linalg.svd(numpy_matrix, full_matrices=False, compute_uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.58708878e+02,   1.20324615e+02,   1.11633514e+02,\n",
       "         8.79006577e+01,   6.69091339e+01,   6.42032242e+01,\n",
       "         5.76826172e+01,   5.68958778e+01,   5.62566757e+01,\n",
       "         5.20814934e+01,   5.02738113e+01,   4.82164955e+01,\n",
       "         4.67037735e+01,   4.41024895e+01,   4.29446640e+01,\n",
       "         4.23494301e+01,   4.18821678e+01,   4.10485306e+01,\n",
       "         3.96737862e+01,   3.86189613e+01,   3.80218163e+01,\n",
       "         3.59379921e+01,   3.50170288e+01,   3.36346245e+01,\n",
       "         3.23577309e+01,   3.13116817e+01,   3.10602493e+01,\n",
       "         3.03501263e+01,   2.98971958e+01,   2.80575581e+01,\n",
       "         2.58904362e+01,   2.56202774e+01,   2.46918049e+01,\n",
       "         2.44935837e+01,   2.38111382e+01,   2.26519260e+01,\n",
       "         2.22668037e+01,   2.21570702e+01,   2.16763878e+01,\n",
       "         2.10580158e+01,   2.10328350e+01,   2.04832916e+01,\n",
       "         1.99380760e+01,   1.89746151e+01,   1.85174484e+01,\n",
       "         1.83655167e+01,   1.76537647e+01,   1.70318909e+01,\n",
       "         1.63343792e+01,   1.60167980e+01,   1.57382212e+01,\n",
       "         1.46689863e+01,   1.42980623e+01,   1.36769781e+01,\n",
       "         1.24078579e+01,   1.19979525e+01,   1.16829071e+01,\n",
       "         1.13290167e+01,   1.10762701e+01,   1.08090391e+01,\n",
       "         1.02711363e+01,   9.86881924e+00,   9.56889629e+00,\n",
       "         8.90388393e+00,   8.80887508e+00,   8.54416561e+00,\n",
       "         8.08788109e+00,   6.67286682e+00,   6.24731159e+00,\n",
       "         5.72558355e+00,   4.00644684e+00,   2.94022584e+00,\n",
       "         1.69518292e+00,   1.66187930e+00,   1.46217430e+00,\n",
       "         1.81085480e-14,   1.16079665e-14,   1.02525639e-14,\n",
       "         8.73220241e-15,   7.92041705e-15,   6.87005172e-15,\n",
       "         5.80204652e-15,   5.49790790e-15,   5.25123454e-15,\n",
       "         4.17373452e-15,   3.89291769e-15,   3.18464504e-15,\n",
       "         2.82667981e-15,   2.61759862e-15,   2.00887691e-15,\n",
       "         1.63796277e-15,   1.39845235e-15,   5.67744003e-16,\n",
       "         4.13812626e-16,   2.17891616e-16,   2.59384066e-31,\n",
       "         1.89713092e-31,   7.05279742e-32,   1.22696065e-32,\n",
       "         0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAFKCAYAAADfWRFiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF8JJREFUeJzt3X+wpXddH/D3Z7OEBIEkoMkKCQnRSpWKIRakxUCKCkFH\nwlgbgbYI2Mp0VKhaIdLpbEadKk4FmVraWhBTRlBAMWBRYgaugoqgIU0CMWAxGCDZGBJIgKrZ3U//\nOM/aszf37t4f3/tr7+s1cybnPOf58TmfPXvy3uf7nO+p7g4AAOu3Z6sLAAA4UQhWAACDCFYAAIMI\nVgAAgwhWAACDCFYAAIMcN1hV1QOr6o+r6sNVdUNV7Z+Wn1FVV1fVzVX17qo6bePLBQDYvmol81hV\n1YO6+0tVdVKSP0jykiT/NMlnu/tnq+rlSc7o7ss3tlwAgO1rRUOB3f2l6e4Dk+xN0kkuTXLltPzK\nJM8eXh0AwA6yomBVVXuq6sNJbk/yu939oSRndfeBJOnu25OcuXFlAgBsfys9Y3W4ux+f5OwkT6yq\nx2Z21uqo1UYXBwCwk+xdzcrdfU9VLSS5JMmBqjqruw9U1b4kdyy1TVUJXADAjtHdtdZtV/KtwC8/\n8o2/qjo1ybcluSnJO5K8YFrte5NcdYwC3Rbd9u/fv+U1bLebnuiLvuiLnujLVt/WayVnrL4yyZVV\ntSezIPZr3f2uqvpAkrdU1YuSfDLJZeuuBgBgBztusOruG5JcuMTyu5J860YUBQCwE5l5fYtcfPHF\nW13CtqMnS9OXpenL0vTl/vRkafqyMVY0Qei6DlDVG30MAIARqiq9kRevAwCwMoIVAMAgghUAwCCC\nFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAgghUA\nwCCCFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAg\nghUAwCCCFQDAIIIVAMAgghUAwCB7N+MgV1xxxVGPzz///Dz/+c/fjEMDAGya6u6NPUBVJ/vnlhzO\nnj3/MYcOHdzQ4wIArFZVpbtrzdtvTrCaP8bB7NlzimAFAGw76w1WrrECABhEsAIAGESwAgAY5LjB\nqqrOrqr3VNVHquqGqvqhafn+qvpUVV073S7Z+HIBALav4168XlX7kuzr7uuq6sFJ/jTJpUm+J8m9\n3f2q42zv4nUAYEdY78Xrx53HqrtvT3L7dP8LVXVTkkceOf5aDwwAcKJZ1TVWVXVekguS/PG06Aer\n6rqqel1VnTa4NgCAHWXFM69Pw4BvS/LS6czVa5P8RHd3Vf1Uklcl+b6lt75i7v5Fay4WAGCkhYWF\nLCwsDNvfiiYIraq9SX4ryW9392uWeP7cJO/s7sct8ZxrrACAHWGzJgj9pSQfnQ9V00XtR3xXkhvX\nWgQAwIlgJd8KfHKS309yQ2annjrJK5I8L7PrrQ4nuSXJi7v7wBLbO2MFAOwIfisQAGAQvxUIALBN\nCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhW\nAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAA\ngwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMIVgAAgwhWAACDCFYAAIMI\nVgAAgwhWAACDHDdYVdXZVfWeqvpIVd1QVS+Zlp9RVVdX1c1V9e6qOm3jywUA2L6qu4+9QtW+JPu6\n+7qqenCSP01yaZIXJvlsd/9sVb08yRndffkS23cyf4yD2bPnlBw6dHDcqwAAGKCq0t211u2Pe8aq\nu2/v7uum+19IclOSszMLV1dOq12Z5NlrLQIA4ESwqmusquq8JBck+UCSs7r7QDILX0nOHF0cAMBO\nsuJgNQ0Dvi3JS6czV4vHEI89pggAcILbu5KVqmpvZqHqjd191bT4QFWd1d0Hpuuw7lh+D1fM3b9o\nbZUCAAy2sLCQhYWFYfs77sXrSVJV/zPJnd39I3PLXpnkru5+pYvXAYATwXovXl/JtwKfnOT3k9yQ\nWULqJK9I8sEkb0lyTpJPJrmsuz+3xPaCFQCwI2x4sFovwQoA2Ck2fLoFAABWRrACABhEsAIAGESw\nAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIA\nGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhE\nsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLACABhEsAIAGESwAgAYRLAC\nABjkuMGqql5fVQeq6vq5Zfur6lNVde10u2RjywQA2P5WcsbqDUmescTyV3X3hdPtdwbXBQCw4xw3\nWHX3+5PcvcRTNb4cAICdaz3XWP1gVV1XVa+rqtOGVQQAsEPtXeN2r03yE93dVfVTSV6V5PuWX/2K\nufsXrfGQAABjLSwsZGFhYdj+qruPv1LVuUne2d2PW81z0/OdzB/jYPbsOSWHDh1ca80AABuiqtLd\na77caaVDgZW5a6qqat/cc9+V5Ma1FgAAcKI47lBgVb0pycVJHl5Vf5lkf5J/UlUXJDmc5JYkL97A\nGgEAdoQVDQWu6wCGAgGAHWKzhgIBADgOwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGAQwQoA\nYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGAQ\nwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEK\nAGAQwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGCQ4warqnp9VR2oquvnlp1RVVdX1c1V9e6q\nOm1jywQA2P5WcsbqDUmesWjZ5Umu6e7HJHlPkh8fXRgAwE5z3GDV3e9PcveixZcmuXK6f2WSZw+u\nCwBgx1nrNVZndveBJOnu25OcOa4kAICdadTF6z1oPwAAO9beNW53oKrO6u4DVbUvyR3HXv2KufsX\nrfGQAABjLSwsZGFhYdj+qvv4J5uq6rwk7+zur58evzLJXd39yqp6eZIzuvvyZbbto09oHcyePafk\n0KGD6y4eAGCkqkp315q3P16wqqo3Jbk4ycOTHEiyP8lvJnlrknOSfDLJZd39uWW2F6wAgB1hw4PV\neglWAMBOsd5gZeZ1AIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCAQQQr\nAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCA\nQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEE\nKwCAQQQrAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEH2rmfjqrolyeeTHE5yX3c/cURRAAA70bqCVWaB\n6uLuvntEMQAAO9l6hwJrwD4AAE4I6w1FneR3q+pDVfWvRxQEALBTrXco8MndfVtVfUVmAeum7n7/\n/Ve7Yu7+Res85Ort23deDhz45P2Wn3XWubn99ls2vR4AYHtYWFjIwsLCsP1Vd4/ZUdX+JPd296sW\nLe/Zia0jDmbPnlNy6NDBIcddYW05uoa/eyajXj8AsPNVVbq71rr9mocCq+pBVfXg6f6XJXl6khvX\nuj8AgJ1uPUOBZyV5++yMVPYm+ZXuvnpMWQAAO8+wocBlD2AoEADYIbZsKBAAgKMJVgAAgwhWAACD\nCFYAAIMIVgAAg5xwwWrfvvNSVUfdlvfA+627b995G1LDcvtdzboAwPZ2wk23sPTUCstPt7DUuuvt\nyXI1LLXf1awLAGws0y0AAGwTghUAwCCCFQDAIIIVAMAgghUAwCCCFQDAIIIVAMAgWxKsDh9+wP0m\nxdzuE2MuNZHnSSd92ZKvYzNrGNG3jdovAOw2WzJBaPKALDdh50ZNzrneCUI3e78rX3f59Vdqo/YL\nADuNCUIBALYJwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGCQHRGslprAcudNXvnAVUwmupp1\nl15/s/uzmj+j9a673MSs2/k9cWK8hwE4nh0xQej6J9HcHhOEbsy6y+9jpX+2IyYIHfFnNKLv23VC\n09W8ZgC2jglCAQC2CcEKAGAQwQoAYBDBCgBgEMEKAGAQwQoAYBDBCgBgEMEKAGCQbRasNnJ28o2o\nYTtb+nUsNWv5avax3Kznq6lh/euu3FIznh/rdSy1fONmel/6Na93v8u95qX2u5p1dyP9GWeze+nP\njsU26xcwtt3M61s/k/lOW3c717b56y5+Px9rVvmNON5yNnMG+dXMpD9i1v0Tmf6Ms9m99GfHYiv9\nBQwzrwMAbBOCFQDAIOsKVlV1SVX9WVV9rKpePqooAICdaM3Bqqr2JPmFJM9I8tgkz62qvz+qsBPf\nwlYXsA0tbHUB29LCwsJWl7At6cvS9GUpC1tdwLbkvbIx1nPG6olJPt7dn+zu+5L8apJLx5S1Gyxs\ndQHb0MJWF7At+fBbmr4sTV+WsrDVBWxL3isbYz3B6pFJbp17/KlpGQDArrR3Mw7y0Id+59yjw7nn\nns04KgDA5lrzPFZV9aQkV3T3JdPjy5N0d79y0XomDAEAdoz1zGO1nmB1UpKbk3xLktuSfDDJc7v7\nprUWAwCwk615KLC7D1XVDya5OrNrtV4vVAEAu9mG/6QNAMBusWEzr5s8dKaqzq6q91TVR6rqhqp6\nybT8jKq6uqpurqp3V9VpW13rVqiqPVV1bVW9Y3q86/tSVadV1Vur6qbpffNNu70vVfXDVXVjVV1f\nVb9SVSfvxp5U1eur6kBVXT+3bNk+VNWPV9XHp/fS07em6o23TF9+dnrd11XVr1fVQ+ee27V9mXvu\nR6vqcFU9bG7Zru5LVf3Q9NpvqKqfmVu+qr5sSLAyeehRDib5ke5+bJJ/lOQHpl5cnuSa7n5Mkvck\n+fEtrHErvTTJR+ce60vymiTv6u6vTfINSf4su7gvVfWIJD+U5MLuflxmlzA8N7uzJ2/I7HN13pJ9\nqKqvS3JZkq9N8swkr63Zr9CeiJbqy9VJHtvdFyT5ePTl71TV2Um+Lckn55Z9bXZxX6rq4iTfmeTr\nu/vrk/ynafmq+7JRZ6xMHjrp7tu7+7rp/heS3JTk7Mz6ceW02pVJnr01FW6d6S/3tyd53dziXd2X\n6V/VF3X3G5Kkuw929+ezy/uS5KQkX1ZVe5OcmuTT2YU96e73J7l70eLl+vCsJL86vYduySxcPHEz\n6txsS/Wlu6/p7sPTww9k9rmb7PK+TF6d5McWLbs0u7sv/ybJz3T3wWmdO6flq+7LRgUrk4cuoarO\nS3JBZn/Jz+ruA8ksfCU5c+sq2zJH/nLPX+i32/vy6CR3VtUbpiHSX6yqB2UX96W7P5Pk55L8ZWaB\n6vPdfU12cU8WOXOZPiz+HP50du/n8IuSvGu6v6v7UlXPSnJrd9+w6Kld3ZckX5PkKVX1gap6b1V9\n47R81X3ZsGusOFpVPTjJ25K8dDpztfhbA7vqWwRV9R1JDkxn8451WnVX9SWzYa4Lk/yX7r4wyRcz\nG+rZte+Xqjo9s381npvkEZmdufrn2cU9OQ59mFNV/z7Jfd395q2uZatV1alJXpFk/1bXsg3tTXJG\ndz8pycuSvHWtO9qoYPXpJI+ae3z2tGxXmoYv3pbkjd191bT4QFWdNT2/L8kdW1XfFnlykmdV1SeS\nvDnJ06rqjUlu3+V9+VRm/5r8k+nxr2cWtHbz++Vbk3yiu+/q7kNJ3p7kH2d392Tecn34dJJz5tbb\ndZ/DVfWCzC43eN7c4t3cl69Kcl6S/11Vf5HZa7+2qs6M/2/fmuQ3kqS7P5TkUFU9PGvoy0YFqw8l\n+eqqOreqTk7ynCTv2KBj7QS/lOSj3f2auWXvSPKC6f73Jrlq8UYnsu5+RXc/qrvPz+z98Z7u/pdJ\n3pnd3ZcDSW6tqq+ZFn1Lko9kd79f/jLJk6rqlOmi0W/J7AsPu7UnlaPP8i7Xh3ckec70DcpHJ/nq\nzCZyPlEd1ZequiSzSw2e1d1/M7feru1Ld9/Y3fu6+/zufnRm/5B7fHffkVlfvmc39mXym0meliTT\n5+/J3f3ZrKUv3b0htySXZDYz+8eTXL5Rx9nut8zOzBxKcl2SDye5durNw5JcM/Xo6iSnb3WtW9ij\npyZ5x3R/1/cls28Cfmh6z/xGktN2e18yG7q4Kcn1mV2g/YDd2JMkb0rymSR/k1ngfGGSM5brQ2bf\nhPvzqXdP3+r6N7kvH8/sW2/XTrfX6kteuOj5TyR5mL7khZkNBb4xyQ1J/iTJU9faFxOEAgAM4uJ1\nAIBBBCsAgEEEKwCAQQQrAIBBBCsAgEEEKwCAQQQr4Jiq6nlV9TubcJxzq+pwVW3559L0W2Ev2uo6\ngJ1nyz/AgK1XVd9cVX9QVZ+rqjur6n1HfoS0u9/U3ZdsUikm1gN2tL1bXQCwtarqIZn9lNCLM/vh\n0ZOTXJTZrMQ7QlWd1LPfEQTYUs5YAV+TpLv7LT3zN919TXffmCRV9b1V9b4jK0/DdS+uqo9V1V1V\n9Qtzz+2pqp+rqr+qqv9TVT8wP7xXVX9RVU+bW3//9OPb91NVL6iqj1bVPVX151X1/XPPPbWqbq2q\nl1XVbZn9Huf8tidX1d1V9XVzy768qr40/ff0qnpnVd1RVZ+d7j9ymTqOqnHxkGVVPbSqXldVn5lq\n+snpNw1TVV9VVQvTmcA7qurNK/sjAXYqwQr4WGa/5P7LVXVJVZ2+xDqLh+i+I8k3Zva7hpdV1dOn\n5d+f5BlJHpfkwiTPXmLb4+37iANJvr27H5rZb3m9uqoumHt+X5LTM/vl+e+f37C7/zbJryd57tzi\ny5IsdPedmX32/VKSc6btv5TkF7K8xTXOP74yyd8mOT/J45N8W5J/NT33k0ne3d2nJzk7yX8+xjGA\nE4BgBbtcd9+b5JuTHE7yi0nuqKqrquorjrHZT3f3vd19a5L3JjkSeP5Zktd0923d/fkkP7OOun67\nu2+Z7r8vsx8YvmhulUNJ9nf3fd291LDlm3N0sHpeZj++mu6+q7vfPp2d+2KSn07ylNXWWFVnJXlm\nkh/u7r+eQtvPJ3nOtMp9Sc6tqkd299929x+u9hjAziJYAenum7v7Rd39qCT/IMkjMgsIyzkwd/9L\nSR483X9Eklvnnpu/vypV9cyq+qNpqO7uzALMl8+t8lfdfd8xdvHeJKdW1ROq6tzMzq69fdr3qVX1\n36vqlqr6XJLfS3L6kSG8VXhUkgckuW0aFr07yX9LciSU/lhmn7MfrKobquqFq9w/sMO4eB04Snd/\nrKp+OYuG11botsyGvI541KLnv5jkQXOP9y21k6o6OcnbkvyLJFd19+GqenuS+eBzzCHGaZu3ZHam\n6kCS35rOTiXJjyb5e0me0N1/VVXfkOTaaf+L97u45q+cu39rkr9O8vDuvl893X1Hpj5W1ZOTXFNV\nv9fdnzhW7cDO5YwV7HJV9Ziq+pEjF29X1TmZDaH90Rp295YkL62qR0zXar1s0fPXJXlOVe2tqn+Y\n5LsXlzP99+TpducUkJ6Z5OlZvTcn+Z7MDQNOHpLk/ya5p6oeluSKY+zjuiRPqapzquq0JJcfeaK7\nb89siPLVVfWQmjm/qp6SJFX13XMXxX8us+HWw2t4HcAOIVgB9yb5piR/XFX3JvnDJNcn+XfLrH+s\nC7n/R2ZB4/okf5rkfyU52N1HwsR/SPLVSe5Ksj/Jryy1r+7+QpKXJHlrVd2V2TVLV632hXX3BzM7\n4/SVSX577qmfz+ws1J2Zvd53LfeauvuaJL82vaYPZTY1xbznZxYCPzq9rrfm/5+Je0Jmfb0nyW8m\necmR68aAE1MtcfYaYIiquiTJf+3uR291LQCbwRkrYJiqOmW66PykaQhsf5Lf2Oq6ADaLM1bAMFV1\nambfsHtMZtcw/VaSfzsN7QGc8AQrAIBBDAUCAAwiWAEADCJYAQAMIlgBAAwiWAEADCJYAQAM8v8A\nzYiYA8W+fNUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b471cc9be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(svd, bins=100)\n",
    "plt.xlabel('Singular values', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-c460bc472f49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Threshold of singular values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of topics'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3152\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3153\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3154\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3155\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3156\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1809\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1810\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1811\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1812\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1422\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must have same first dimension\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAE4CAYAAACHeo0bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD45JREFUeJzt3V2I5Xd9x/HPN9mGoo2BGAiYGGl9ICg+VGrMRaCjKc3G\nm4hXSSDFgDRQI94ZvRBHEKx3IvGBpUHxQiKYgmmrmCIZSmiiKeRB665JVGKyCREfoYKwhm8vZgzj\nuDtzdvY7O3uyrxccOP9zfvM/P/gxkzf/89t/qrsDAMCcc/Z7AgAALzYCCwBgmMACABgmsAAAhgks\nAIBhAgsAYNiOgVVVd1TVc1X16DZjPlNVj1fVw1X1ltkpAgAsl0WuYH0xyTUnerOqrk3y6u5+bZJb\nknxhaG4AAEtpx8Dq7vuS/GqbIdcl+fLG2O8kuaCqLp6ZHgDA8pnYg3VJkqc2HR/deA0A4KxkkzsA\nwLADA+c4muSVm44v3XjtT1SV//EhALA0urt283OLBlZtPI7n7iTvT/LVqroyya+7+7kTncj/XHp5\nra6uZnV1db+nwS5Yu+Vm/Zab9VteVbtqqyQLBFZVfSXJSpKXV9VPk3wsyXlJursPdfc3qupdVfVE\nkt8muXnXswEAeBHYMbC6+8YFxtw6Mx0AgOVnkzsLW1lZ2e8psEvWbrlZv+Vm/c5OdTr3RFVV24MF\nACyDqtr1JndXsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGCSwAgGEC\nCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGCSwAgGEC\nCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGCSwAgGEC\nCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGLRRYVXWw\nqo5U1WNVddtx3n9ZVd1dVQ9X1feq6r3jMwUAWBLV3dsPqDonyWNJrk7yTJIHk1zf3Uc2jflIkpd1\n90eq6qIkP0xycXf/fsu5eqfPAwA4E1RVurt287OLXMG6Isnj3f1kdx9LcmeS67aM6STnbzw/P8kv\ntsYVAMDZYpHAuiTJU5uOn954bbPbk7y+qp5J8kiSD85MDwBg+Uxtcr8myUPd/Yokf53ks1X1F0Pn\nBgBYKgcWGHM0yWWbji/deG2zm5N8Mkm6+0dV9ZMklyf5n60nW11dfeH5yspKVlZWTmrCAAB7YW1t\nLWtrayPnWmST+7lZ37R+dZJnk3w3yQ3dfXjTmM8m+Vl3f7yqLs56WL25u3+55Vw2uQMAS+FUNrnv\neAWru5+vqluT3JP1rxTv6O7DVXXL+tt9KMknknypqh7d+LEPbY0rAICzxY5XsEY/zBUsAGBJ7PVt\nGgAAOAkCCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACG\nCSwAgGECCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACG\nCSwAgGECCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACG\nCSwAgGECCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGLZQYFXV\nwao6UlWPVdVtJxizUlUPVdX3q+re2WkCACyP6u7tB1Sdk+SxJFcneSbJg0mu7+4jm8ZckOS/k/x9\ndx+tqou6++fHOVfv9HkAAGeCqkp3125+dpErWFckeby7n+zuY0nuTHLdljE3Jrmru48myfHiCgDg\nbLFIYF2S5KlNx09vvLbZ65JcWFX3VtWDVXXT1AQBAJbNgcHzvDXJO5O8NMn9VXV/dz+xdeDq6uoL\nz1dWVrKysjI0BQCA3VtbW8va2trIuRbZg3VlktXuPrhx/OEk3d2f2jTmtiR/3t0f3zj+lyTf7O67\ntpzLHiwAYCns9R6sB5O8pqpeVVXnJbk+yd1bxnw9yVVVdW5VvSTJ25Mc3s2EAACW3Y5fEXb381V1\na5J7sh5kd3T34aq6Zf3tPtTdR6rqW0keTfJ8kkPd/YM9nTkAwBlqx68IRz/MV4QAwJLY668IAQA4\nCQILAGCYwAIAGCawAACGCSwAgGECCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACA\nYQILAGCYwAIAGCawAACGCSwAgGECCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACA\nYQILAGCYwAIAGCawAACGCSwAgGECCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACA\nYQILAGCYwAIAGCawAACGCSwAgGECCwBgmMACABgmsAAAhgksAIBhAgsAYJjAAgAYtlBgVdXBqjpS\nVY9V1W3bjHtbVR2rqvfMTREAYLnsGFhVdU6S25Nck+QNSW6oqstPMO6fk3xrepIAAMtkkStYVyR5\nvLuf7O5jSe5Mct1xxn0gydeS/GxwfgAAS2eRwLokyVObjp/eeO0FVfWKJO/u7s8nqbnpAQAsnwND\n5/l0ks17s04YWaurqy88X1lZycrKytAUAAB2b21tLWtrayPnqu7efkDVlUlWu/vgxvGHk3R3f2rT\nmB//4WmSi5L8Nsk/dvfdW87VO30eAMCZoKrS3bv6Zm6RwDo3yQ+TXJ3k2STfTXJDdx8+wfgvJvm3\n7v7X47wnsACApXAqgbXjV4Td/XxV3Zrknqzv2bqjuw9X1S3rb/ehrT+ym4kAALxY7HgFa/TDXMEC\nAJbEqVzBcid3AIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGCSwAgGECCwBgmMAC\nABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGCSwAgGECCwBgmMAC\nABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGCSwAgGECCwBgmMAC\nABgmsAAAhgksAIBhAgsAYJjAAgAYJrAAAIYJLACAYQILAGCYwAIAGCawAACGCSwAgGECCwBg2EKB\nVVUHq+pIVT1WVbcd5/0bq+qRjcd9VfXG+akCACyH6u7tB1Sdk+SxJFcneSbJg0mu7+4jm8ZcmeRw\nd/+mqg4mWe3uK49zrt7p8wAAzgRVle6u3fzsIlewrkjyeHc/2d3HktyZ5LrNA7r7ge7+zcbhA0ku\n2c1kAABeDBYJrEuSPLXp+OlsH1DvS/LNU5kUAMAyOzB5sqp6R5Kbk1w1eV4AgGWySGAdTXLZpuNL\nN177I1X1piSHkhzs7l+d6GSrq6svPF9ZWcnKysqCUwUA2Dtra2tZW1sbOdcim9zPTfLDrG9yfzbJ\nd5Pc0N2HN425LMm3k9zU3Q9scy6b3AGApXAqm9x3vILV3c9X1a1J7sn6nq07uvtwVd2y/nYfSvLR\nJBcm+VxVVZJj3X3FbiYEALDsdryCNfphrmABAEtir2/TAADASRBYAADDBBYAwDCBBQAwTGABAAwT\nWAAAwwQWAMAwgQUAMExgAQAME1gAAMMEFgDAMIEFADBMYAEADBNYAADDBBYAwDCBBQAwTGABAAwT\nWAAAwwQWAMAwgQUAMExgAQAME1gAAMMEFgDAMIEFADBMYAEADBNYAADDBBYAwDCBBQAwTGABAAwT\nWAAAwwQWAMAwgQUAMExgAQAME1gAAMMEFgDAMIEFADBMYAEADBNYAADDBBYAwDCBBQAwTGABAAwT\nWAAAwwQWAMAwgQUAMExgAQAME1gAAMMEFgDAsIUCq6oOVtWRqnqsqm47wZjPVNXjVfVwVb1ldpoA\nAMtjx8CqqnOS3J7kmiRvSHJDVV2+Zcy1SV7d3a9NckuSL+zBXNlna2tr+z0FdsnaLTfrt9ys39lp\nkStYVyR5vLuf7O5jSe5Mct2WMdcl+XKSdPd3klxQVRePzpR954/E8rJ2y836LTfrd3ZaJLAuSfLU\npuOnN17bbszR44wBADgr2OQOADCsunv7AVVXJlnt7oMbxx9O0t39qU1jvpDk3u7+6sbxkSR/293P\nbTnX9h8GAHAG6e7azc8dWGDMg0leU1WvSvJskuuT3LBlzN1J3p/kqxtB9uutcXUqkwQAWCY7BlZ3\nP19Vtya5J+tfKd7R3Yer6pb1t/tQd3+jqt5VVU8k+W2Sm/d22gAAZ64dvyIEAODk7MkmdzcmXV47\nrV1V3VhVj2w87quqN+7HPDm+RX73Nsa9raqOVdV7Tuf82N6CfztXquqhqvp+Vd17uufI8S3wt/Nl\nVXX3xn/zvldV792HaXIcVXVHVT1XVY9uM+bkm6W7Rx9Zj7YnkrwqyZ8leTjJ5VvGXJvkPzaevz3J\nA9Pz8NiztbsyyQUbzw9auzPnscj6bRr37ST/nuQ9+z1vj8XXL8kFSf43ySUbxxft97w9Fl67jyT5\n5B/WLckvkhzY77l7dJJcleQtSR49wfu7apa9uILlxqTLa8e16+4Huvs3G4cPxP3OziSL/O4lyQeS\nfC3Jz07n5NjRIut3Y5K7uvtoknT3z0/zHDm+Rdauk5y/8fz8JL/o7t+fxjlyAt19X5JfbTNkV82y\nF4HlxqTLa5G12+x9Sb65pzPiZOy4flX1iiTv7u7PJ/Gves8si/z+vS7JhVV1b1U9WFU3nbbZsZ1F\n1u72JK+vqmeSPJLkg6dpbpy6XTXLIrdpgD9RVe/I+r8WvWq/58JJ+XSSzftDRNZyOZDkrUnemeSl\nSe6vqvu7+4n9nRYLuCbJQ939zqp6dZL/rKo3dff/7ffE2Bt7EVhHk1y26fjSjde2jnnlDmM4/RZZ\nu1TVm5IcSnKwu7e7rMrptcj6/U2SO6uqsr4P5NqqOtbdd5+mOXJii6zf00l+3t2/S/K7qvqvJG/O\n+v4f9s8ia3dzkk8mSXf/qKp+kuTyJP9zWmbIqdhVs+zFV4Qv3Ji0qs7L+o1Jt/7xvjvJPyQv3Cn+\nuDcm5bTbce2q6rIkdyW5qbt/tA9z5MR2XL/u/quNx19mfR/WP4mrM8Yifzu/nuSqqjq3ql6S9Q23\nh0/zPPlTi6zdk0n+Lkk29u+8LsmPT+ss2U7lxFf0d9Us41ew2o1Jl9Yia5fko0kuTPK5jasgx7r7\niv2bNX+w4Pr90Y+c9klyQgv+7TxSVd9K8miS55Mc6u4f7OO0ycK/e59I8qVNtwL4UHf/cp+mzCZV\n9ZUkK0leXlU/TfKxJOflFJvFjUYBAIbtyY1GAQDOZgILAGCYwAIAGCawAACGCSwAgGECCwBgmMAC\nABgmsAAAhv0/Y6GJlTjcH+8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b4717bf668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(0,101,5), svd, '*-')\n",
    "plt.xlabel('Threshold of singular values', fontsize=12)\n",
    "plt.ylabel('Number of topics', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente asignamos el valor a una constante, junto a los otros dos parámetros comentados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7e26c447a70f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlsi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lsi' is not defined"
     ]
    }
   ],
   "source": [
    "lsi.projection.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write out coordinates to file\n",
    "fcoords = open(\"data/coords.csv\", 'wb')\n",
    "for vector in lsi[corpus]:\n",
    "    if len(vector) != 2:\n",
    "        continue\n",
    "    fcoords.write(\"%6.4f\\t%6.4f\\n\" % (vector[0][1], vector[1][1]))\n",
    "fcoords.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#POC\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOTAL_LSA_TOPICS = 50\n",
    "SIMILARITY_THRESHOLD = 0.6\n",
    "LIBRARY_COINCIDENCE_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo LSA: Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=TOTAL_LSA_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Podemos ver como influyen las palabras en la determinación de los diferentes tópicos, por ejemplo mostramos los 3 primeros tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA(k=20)\n",
      "Training LDA(k=30)\n",
      "Training LDA(k=40)\n",
      "Training LDA(k=50)\n",
      "Training LDA(k=60)\n",
      "Training LDA(k=70)\n",
      "Training LDA(k=80)\n",
      "Training LDA(k=90)\n",
      "Training LDA(k=100)\n"
     ]
    }
   ],
   "source": [
    "#POC\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "trained_models = OrderedDict()\n",
    "for num_topics in range(20, 101, 10):\n",
    "    print(\"Training LDA(k=%d)\" % num_topics)\n",
    "    lda = models.LdaMulticore(\n",
    "        corpus_tfidf, id2word=dictionary, num_topics=num_topics, workers=4,\n",
    "        passes=10, iterations=100, random_state=42, \n",
    "        eval_every=None, # Don't evaluate model perplexity, takes too much time.\n",
    "        alpha='asymmetric',  # shown to be better than symmetric in most cases\n",
    "        decay=0.5, offset=64  # best params from Hoffman paper\n",
    "    )\n",
    "    trained_models[num_topics] = lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(20, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3953c048>),\n",
       "             (30, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3933a940>),\n",
       "             (40, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3a45e780>),\n",
       "             (50, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3a44c0f0>),\n",
       "             (60, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3a583128>),\n",
       "             (70, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3933ad30>),\n",
       "             (80, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3a4532b0>),\n",
       "             (90, <gensim.models.ldamulticore.LdaMulticore at 0x1ef3a247898>),\n",
       "             (100,\n",
       "              <gensim.models.ldamulticore.LdaMulticore at 0x1ef3a45ed68>)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamulticore.LdaMulticore at 0x1ef3953c048>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_lda = [dictionary.doc2bow(text) for text in texts_from_readme]\n",
    "model_lda = models.LdaModel(corpus_lda, id2word=dictionary, num_topics=20)\n",
    "badcm = CoherenceModel(model=model_lda, texts=texts_from_readme, dictionary=dictionary, coherence='c_v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5, 5)],\n",
       " [],\n",
       " [(3, 1), (4, 1)],\n",
       " [],\n",
       " [],\n",
       " [(4, 2), (5, 1)],\n",
       " [(4, 1)],\n",
       " [(3, 1), (5, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(4, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 1), (10, 1)],\n",
       " [(4, 1), (5, 2)],\n",
       " [(3, 3)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 2)],\n",
       " [(3, 1)],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [(0, 2), (3, 6), (5, 1)],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [(4, 2), (5, 13)],\n",
       " [],\n",
       " [(3, 10), (4, 1), (5, 2)],\n",
       " [(5, 2)],\n",
       " [],\n",
       " [(3, 2)],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [(3, 1), (4, 1)],\n",
       " [],\n",
       " [],\n",
       " [(3, 2), (5, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 2), (4, 3), (5, 1)],\n",
       " [(5, 1)],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 3)],\n",
       " [],\n",
       " [],\n",
       " [(3, 1), (4, 1)],\n",
       " [(4, 1)],\n",
       " [(3, 1), (4, 1), (5, 1)],\n",
       " [(3, 1)],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [(4, 1)],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 2)],\n",
       " [],\n",
       " [(4, 1)],\n",
       " [(4, 1), (5, 2)],\n",
       " [],\n",
       " [(5, 2)],\n",
       " [(4, 3)],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(3, 3)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [(5, 2)],\n",
       " [(3, 1)],\n",
       " []]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'glue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-5b7db2b5d142>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbadcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msliding_windows_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             per_topic_postings, num_windows = measure.prob(texts=self.texts, segmented_topics=segmented_topics,\n\u001b[1;32m--> 207\u001b[1;33m                                                            dictionary=self.dictionary, window_size=self.window_size)\n\u001b[0m\u001b[0;32m    208\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoherence\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c_v'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mconfirmed_measures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmented_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_topic_postings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nlr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_windows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[1;34m(texts, segmented_topics, dictionary, window_size)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mwindow_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_topic_postings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_topic_posting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_topic_postings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken2id_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yazquez\\Anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\probability_estimation.py\u001b[0m in \u001b[0;36madd_topic_posting\u001b[1;34m(top_ids, window, per_topic_postings, window_id, token2id_dict)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_topic_posting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_topic_postings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken2id_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mword_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken2id_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mper_topic_postings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'glue'"
     ]
    }
   ],
   "source": [
    "print(badcm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.381629389937\n",
      "0.374244785753\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    import pyLDAvis.gensim\n",
    "    CAN_VISUALIZE = True\n",
    "    pyLDAvis.enable_notebook()\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    ValueError(\"SKIP: please install pyLDAvis\")\n",
    "    CAN_VISUALIZE = False\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, HdpModel\n",
    "from gensim.models.wrappers import LdaVowpalWabbit, LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
    "\n",
    "\n",
    "texts = [['human', 'interface', 'computer'],\n",
    "         ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "         ['eps', 'user', 'interface', 'system'],\n",
    "         ['system', 'human', 'system', 'eps'],\n",
    "         ['user', 'response', 'time'],\n",
    "         ['trees'],\n",
    "         ['graph', 'trees'],\n",
    "         ['graph', 'minors', 'trees'],\n",
    "         ['graph', 'minors', 'survey']]\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "goodLdaModel = LdaModel(corpus=corpus, id2word=dictionary, iterations=50, num_topics=2)\n",
    "badLdaModel = LdaModel(corpus=corpus, id2word=dictionary, iterations=1, num_topics=2)\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "goodcm = CoherenceModel(model=goodLdaModel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "badcm = CoherenceModel(model=badLdaModel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "print(goodcm.get_coherence())\n",
    "print(badcm.get_coherence())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'CoherenceModel' has no attribute 'for_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-006305ba6fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# This performs a single pass over the reference corpus, accumulating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# the necessary statistics for all of the models at once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m cm = models.CoherenceModel.for_models(\n\u001b[0m\u001b[0;32m      7\u001b[0m     trained_models.values(), dictionary, texts=corpus.get_texts(), coherence='c_v')\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'CoherenceModel' has no attribute 'for_models'"
     ]
    }
   ],
   "source": [
    "from gensim import utils, models\n",
    "# Now estimate the probabilities for the CoherenceModel.\n",
    "# This performs a single pass over the reference corpus, accumulating\n",
    "# the necessary statistics for all of the models at once.\n",
    "cm = models.CoherenceModel.for_models(\n",
    "    trained_models.values(), dictionary, texts=corpus.get_texts(), coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "0.001*\"worker\"\n",
      "0.001*\"compon\"\n",
      "0.001*\"task\"\n",
      "0.001*\"systemautopsi\"\n",
      "0.001*\"camelot\"\n",
      "0.001*\"pair\"\n",
      "0.001*\"replac\"\n",
      "0.001*\"liveview\"\n",
      "0.001*\"devic\"\n",
      "0.001*\"regreplac\"\n",
      "\n",
      "Topic 1:\n",
      "0.002*\"transifex\"\n",
      "0.001*\"temporari\"\n",
      "0.001*\"kbd\"\n",
      "0.001*\"rpc\"\n",
      "0.001*\"paip\"\n",
      "0.001*\"glue\"\n",
      "0.001*\"todo\"\n",
      "0.001*\"enter\"\n",
      "0.001*\"omd\"\n",
      "0.001*\"evap\"\n",
      "\n",
      "Topic 2:\n",
      "0.001*\"citeproc\"\n",
      "0.001*\"rst\"\n",
      "0.001*\"mxunit\"\n",
      "0.001*\"nginx\"\n",
      "0.001*\"wtop\"\n",
      "0.001*\"ms\"\n",
      "0.001*\"0\"\n",
      "0.001*\"heroku\"\n",
      "0.001*\"sentri\"\n",
      "0.001*\"index\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(model.print_topics(3)):\n",
    "    print('Topic {}:'.format(i))\n",
    "    print(topic[1].replace(' + ', '\\n'))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando nuestro modelo LSI construimos la matriz de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_matrix = similarities.MatrixSimilarity(model[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función auxiliar que dado un proyecto, nos determina la lista de proyectos que superan el umbral de similitud. Para cada proyecto que supere el umbral, almacenaremos el índice dentro de la matriz de proyectos, para localizarla posteriormente, y el grado de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarities(doc, model):\n",
    "    ''' Calcula las similitudes de un documento, expresado este como una lista de palabras'''\n",
    "    project_similarities = []\n",
    "\n",
    "    # Convertimos el documento al espacio LSI\n",
    "    vec_bow = dictionary.doc2bow(doc)\n",
    "    vec_lsi = model[vec_bow]\n",
    "\n",
    "    similarities = similarity_matrix[vec_lsi]\n",
    "    similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    "\n",
    "    for sim in similarities:\n",
    "        similarity_project = int(sim[0])\n",
    "        similarity_score = sim[1]\n",
    "        if similarity_score > SIMILARITY_THRESHOLD:\n",
    "            project_similarities.append((similarity_project, similarity_score))\n",
    "\n",
    "    return (project_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29, 0.17975067402715453), (33, 0.6155647936254147), (35, 0.2946657501113738), (156, 0.4907980295142131), (190, 0.5109393995103052)]\n",
      "[(0, 0.067016973374094749), (1, 0.065491396317614081), (2, 0.73507562960905548), (3, 0.066668520310089616), (4, 0.065747480389146185)]\n"
     ]
    }
   ],
   "source": [
    "#POC\n",
    "def transform_text(text):\n",
    "    my_tfidf_transformer = models.TfidfModel(dictionary=dictionary)\n",
    "    tokens = get_words(text)\n",
    "    vec_bow = dictionary.doc2bow(tokens)\n",
    "    return(my_tfidf_transformer[vec_bow])\n",
    "\n",
    "test_vector = transform_text(\"works on Windows and write down some installation instructions\")\n",
    "\n",
    "print(test_vector)\n",
    "\n",
    "my_lda_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=TOTAL_LSA_TOPICS)\n",
    "test_vector_topics = my_lda_model.get_document_topics(test_vector)\n",
    "\n",
    "print(test_vector_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for x in [29, 33, 35, 156, 31]:\n",
    "    print(my_lda_model.get_term_topics(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, como prueba de concepto, vamos a determinar los proyectos similares a uno dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poc_readme_doc = get_words(\"works on Windows and write down some installation instructions\")\n",
    "poc_library = ['sys', 'os', 'json', 'codecs', 'shutil']\n",
    "project_similarities = get_similarities(poc_readme_doc, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos los 10 proyectos más similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: sharepastes: - Similarity: 0.999998152256012\n",
      "Project: django-maintenancemode: - Similarity: 0.9999943375587463\n",
      "Project: Sublime-Text-2-Goto-CSS-Declaration: - Similarity: 0.9999935030937195\n",
      "Project: EvaP: - Similarity: 0.9999876618385315\n",
      "Project: orderedmultidict: - Similarity: 0.9999721050262451\n",
      "Project: paip-python: - Similarity: 0.999944806098938\n",
      "Project: SublimeFormatSQL: - Similarity: 0.9999352097511292\n",
      "Project: python-transifex: - Similarity: 0.9998762011528015\n",
      "Project: ColdFusion: - Similarity: 0.999850332736969\n",
      "Project: glue: - Similarity: 0.9997751712799072\n"
     ]
    }
   ],
   "source": [
    "for similarity in project_similarities[:10]:\n",
    "    print(\"Project: {0}: - Similarity: {1}\".format(projects[similarity[0]][\"name\"], similarity[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "El objetivo final no no es encontrar proyectos similares, sino encontrar las librerias que usan esos proyectos similares. Así pues, recorremos esos proyectos e identificamos esas librerías, descartando las que nuestro proyecto ya incluye.\n",
    "\n",
    "Creamos un diccionario donde la clave es cada libreria y el valor el scoring de esa librería, obtendremos dicho scoring sumando el scoring de similitud de cada proyecto en el que encontremos la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "library_similarities=defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recorremos los 10 proyectos más similares y calculamos el scoring de cada librería usada en esos proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for similarity in project_similarities[:10]:\n",
    "    project_similarity = similarity[1]\n",
    "    project_libraries = projects[similarity[0]][\"library\"]\n",
    "    for library in project_libraries:\n",
    "        if library not in poc_library:\n",
    "            library_similarities[library] += project_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las 10 librerías más usadas por los proyectos similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library: re: - Score: 6.99940150976181\n",
      "Library: unittest: - Score: 3.999568283557892\n",
      "Library: __future__: - Score: 2.9998949766159058\n",
      "Library: itertools: - Score: 2.9998949766159058\n",
      "Library: logging: - Score: 2.9997076392173767\n",
      "Library: django.contrib.auth.models: - Score: 1.9999819993972778\n",
      "Library: django.conf.urls: - Score: 1.9999819993972778\n",
      "Library: django.core.management: - Score: 1.9999819993972778\n",
      "Library: django: - Score: 1.9999819993972778\n",
      "Library: django.conf: - Score: 1.9999819993972778\n"
     ]
    }
   ],
   "source": [
    "for library in sorted(library_similarities, key=library_similarities.get, reverse=True)[:10]:\n",
    "    print(\"Library: {0}: - Score: {1}\".format(library, library_similarities[library]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x1b471083b00>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
